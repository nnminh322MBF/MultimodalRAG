{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c07e2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(5,6,7,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba3bbff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1, 5, 1, 5, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a.shape[0], + 1,)*(a.ndim-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b4585a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.ndim-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "308a94f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa  = torch.rand((4,1,1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed1fb122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.8734, 0.7546, 0.4107,  ..., 0.7972, 0.9881, 0.7353],\n",
       "          [0.7520, 0.4873, 0.3946,  ..., 0.3766, 0.7400, 0.4296],\n",
       "          [0.3668, 0.4034, 0.2728,  ..., 0.9406, 0.3254, 0.5981],\n",
       "          ...,\n",
       "          [0.5145, 0.1129, 0.0303,  ..., 0.3213, 0.2042, 0.1613],\n",
       "          [0.8166, 0.6152, 0.0367,  ..., 0.9805, 0.3592, 0.6301],\n",
       "          [0.4300, 0.4130, 0.5088,  ..., 0.7145, 0.7846, 0.8763]],\n",
       "\n",
       "         [[0.1773, 0.2812, 0.4533,  ..., 0.1468, 0.6121, 0.3595],\n",
       "          [0.4660, 0.2312, 0.8779,  ..., 0.9052, 0.0113, 0.7876],\n",
       "          [0.9612, 0.0249, 0.1306,  ..., 0.3103, 0.2675, 0.4631],\n",
       "          ...,\n",
       "          [0.0020, 0.6623, 0.5289,  ..., 0.6446, 0.1159, 0.6806],\n",
       "          [0.5992, 0.1158, 0.1142,  ..., 0.5587, 0.5736, 0.9890],\n",
       "          [0.5014, 0.3853, 0.3451,  ..., 0.2594, 0.1131, 0.5614]],\n",
       "\n",
       "         [[0.8982, 0.4145, 0.0793,  ..., 0.3067, 0.0753, 0.0052],\n",
       "          [0.7063, 0.6566, 0.4647,  ..., 0.7077, 0.6467, 0.9491],\n",
       "          [0.1051, 0.7757, 0.3995,  ..., 0.8239, 0.1163, 0.9325],\n",
       "          ...,\n",
       "          [0.7953, 0.3016, 0.3938,  ..., 0.8628, 0.7680, 0.5851],\n",
       "          [0.0733, 0.4482, 0.0909,  ..., 0.0966, 0.6869, 0.0167],\n",
       "          [0.2836, 0.2668, 0.0030,  ..., 0.3772, 0.9549, 0.9191]],\n",
       "\n",
       "         [[0.9783, 0.7525, 0.5051,  ..., 0.4349, 0.8226, 0.9677],\n",
       "          [0.5262, 0.6295, 0.8796,  ..., 0.6891, 0.8179, 0.1023],\n",
       "          [0.5634, 0.1641, 0.0087,  ..., 0.1414, 0.3874, 0.9183],\n",
       "          ...,\n",
       "          [0.7195, 0.0799, 0.6887,  ..., 0.4403, 0.9394, 0.1361],\n",
       "          [0.1350, 0.8942, 0.0460,  ..., 0.3685, 0.5149, 0.8238],\n",
       "          [0.4249, 0.2232, 0.7208,  ..., 0.1319, 0.5835, 0.7180]],\n",
       "\n",
       "         [[0.5852, 0.2951, 0.4910,  ..., 0.4691, 0.9526, 0.1855],\n",
       "          [0.8300, 0.2650, 0.1249,  ..., 0.1744, 0.0786, 0.9382],\n",
       "          [0.5057, 0.5428, 0.3606,  ..., 0.4407, 0.4104, 0.3700],\n",
       "          ...,\n",
       "          [0.1537, 0.0802, 0.8098,  ..., 0.8106, 0.5786, 0.8326],\n",
       "          [0.7971, 0.7727, 0.0349,  ..., 0.5214, 0.8220, 0.1345],\n",
       "          [0.0738, 0.7601, 0.6284,  ..., 0.9213, 0.4943, 0.3333]],\n",
       "\n",
       "         [[0.7490, 0.3682, 0.4325,  ..., 0.9572, 0.9071, 0.6384],\n",
       "          [0.0171, 0.5384, 0.9432,  ..., 0.9637, 0.5935, 0.0295],\n",
       "          [0.4043, 0.5595, 0.8656,  ..., 0.4095, 0.4757, 0.5508],\n",
       "          ...,\n",
       "          [0.2945, 0.0949, 0.1485,  ..., 0.3106, 0.3772, 0.6512],\n",
       "          [0.2747, 0.8246, 0.3706,  ..., 0.6331, 0.9693, 0.3103],\n",
       "          [0.7269, 0.9535, 0.9518,  ..., 0.9950, 0.0957, 0.3346]]],\n",
       "\n",
       "\n",
       "        [[[0.7454, 0.6111, 0.4287,  ..., 0.9811, 0.8628, 0.8116],\n",
       "          [0.7874, 0.9042, 0.8369,  ..., 0.7258, 0.2537, 0.9716],\n",
       "          [0.3754, 0.6548, 0.2052,  ..., 0.3262, 0.1239, 0.2900],\n",
       "          ...,\n",
       "          [0.3902, 0.6004, 0.2091,  ..., 0.5879, 0.4902, 0.2476],\n",
       "          [0.8936, 0.3453, 0.0355,  ..., 0.8974, 0.6521, 0.1821],\n",
       "          [0.9671, 0.1108, 0.2177,  ..., 0.4638, 0.7848, 0.6004]],\n",
       "\n",
       "         [[0.9751, 0.7134, 0.9318,  ..., 0.6055, 0.7462, 0.2655],\n",
       "          [0.1094, 0.7085, 0.2857,  ..., 0.8819, 0.7975, 0.9880],\n",
       "          [0.2055, 0.3119, 0.8421,  ..., 0.5060, 0.3095, 0.9758],\n",
       "          ...,\n",
       "          [0.0130, 0.7122, 0.7124,  ..., 0.5036, 0.7726, 0.0954],\n",
       "          [0.7507, 0.7730, 0.9423,  ..., 0.9566, 0.7713, 0.6654],\n",
       "          [0.4293, 0.7355, 0.2995,  ..., 0.4427, 0.8655, 0.4300]],\n",
       "\n",
       "         [[0.1035, 0.0996, 0.2182,  ..., 0.0381, 0.4503, 0.6144],\n",
       "          [0.5839, 0.0703, 0.8892,  ..., 0.6330, 0.9519, 0.6939],\n",
       "          [0.8951, 0.9618, 0.0249,  ..., 0.4438, 0.2708, 0.7322],\n",
       "          ...,\n",
       "          [0.1262, 0.7453, 0.6720,  ..., 0.8727, 0.8457, 0.1423],\n",
       "          [0.0586, 0.5925, 0.5051,  ..., 0.0787, 0.1688, 0.9813],\n",
       "          [0.5861, 0.8308, 0.2740,  ..., 0.1712, 0.9059, 0.8078]],\n",
       "\n",
       "         [[0.4211, 0.4409, 0.6206,  ..., 0.0242, 0.4554, 0.4392],\n",
       "          [0.0637, 0.3546, 0.2498,  ..., 0.0736, 0.3418, 0.4732],\n",
       "          [0.3094, 0.0316, 0.0549,  ..., 0.0787, 0.9982, 0.3970],\n",
       "          ...,\n",
       "          [0.8186, 0.2525, 0.6920,  ..., 0.9799, 0.6579, 0.4724],\n",
       "          [0.8039, 0.6993, 0.2122,  ..., 0.8987, 0.0458, 0.7125],\n",
       "          [0.5384, 0.0023, 0.9002,  ..., 0.9864, 0.1874, 0.8747]],\n",
       "\n",
       "         [[0.9783, 0.3904, 0.6435,  ..., 0.8188, 0.6581, 0.0456],\n",
       "          [0.3060, 0.5306, 0.8546,  ..., 0.1719, 0.2319, 0.4335],\n",
       "          [0.2943, 0.0105, 0.7009,  ..., 0.5045, 0.4198, 0.3229],\n",
       "          ...,\n",
       "          [0.8407, 0.7256, 0.8788,  ..., 0.1651, 0.2738, 0.2064],\n",
       "          [0.8888, 0.8233, 0.2531,  ..., 0.3544, 0.3082, 0.6632],\n",
       "          [0.4581, 0.3974, 0.3824,  ..., 0.3293, 0.1151, 0.5311]],\n",
       "\n",
       "         [[0.1225, 0.9834, 0.6048,  ..., 0.1084, 0.1244, 0.3449],\n",
       "          [0.9326, 0.3818, 0.2493,  ..., 0.8572, 0.5143, 0.3092],\n",
       "          [0.6888, 0.0199, 0.5935,  ..., 0.3652, 0.4426, 0.9986],\n",
       "          ...,\n",
       "          [0.7368, 0.1998, 0.7566,  ..., 0.8114, 0.1747, 0.8819],\n",
       "          [0.1434, 0.2970, 0.0771,  ..., 0.3509, 0.8396, 0.2131],\n",
       "          [0.0939, 0.1519, 0.2397,  ..., 0.1186, 0.0926, 0.1078]]],\n",
       "\n",
       "\n",
       "        [[[0.5574, 0.4659, 0.6285,  ..., 0.9256, 0.3846, 0.6891],\n",
       "          [0.7343, 0.8242, 0.0375,  ..., 0.4989, 0.8063, 0.0801],\n",
       "          [0.3391, 0.8086, 0.9332,  ..., 0.7096, 0.1179, 0.1982],\n",
       "          ...,\n",
       "          [0.0119, 0.2884, 0.8530,  ..., 0.3074, 0.5340, 0.2136],\n",
       "          [0.5958, 0.9127, 0.5559,  ..., 0.7963, 0.6737, 0.1941],\n",
       "          [0.7026, 0.8543, 0.7076,  ..., 0.2501, 0.4491, 0.6660]],\n",
       "\n",
       "         [[0.2955, 0.9784, 0.7819,  ..., 0.3840, 0.9743, 0.8352],\n",
       "          [0.8396, 0.3690, 0.3359,  ..., 0.1852, 0.3925, 0.8829],\n",
       "          [0.0107, 0.1765, 0.4025,  ..., 0.5865, 0.1679, 0.7702],\n",
       "          ...,\n",
       "          [0.3215, 0.8131, 0.6691,  ..., 0.3602, 0.0199, 0.0575],\n",
       "          [0.7808, 0.5002, 0.7409,  ..., 0.7280, 0.2897, 0.7696],\n",
       "          [0.2836, 0.9579, 0.8780,  ..., 0.1383, 0.1648, 0.7653]],\n",
       "\n",
       "         [[0.3487, 0.8044, 0.0273,  ..., 0.7632, 0.7389, 0.2756],\n",
       "          [0.2338, 0.5300, 0.6765,  ..., 0.4164, 0.5733, 0.0552],\n",
       "          [0.7305, 0.0508, 0.7930,  ..., 0.9697, 0.7461, 0.6948],\n",
       "          ...,\n",
       "          [0.2311, 0.0298, 0.1854,  ..., 0.5999, 0.6731, 0.2619],\n",
       "          [0.0741, 0.5772, 0.0991,  ..., 0.8662, 0.8332, 0.3843],\n",
       "          [0.9099, 0.0867, 0.3828,  ..., 0.5796, 0.7542, 0.0615]],\n",
       "\n",
       "         [[0.4982, 0.8964, 0.7136,  ..., 0.4400, 0.7181, 0.0697],\n",
       "          [0.0021, 0.0184, 0.2899,  ..., 0.9576, 0.9164, 0.2012],\n",
       "          [0.0545, 0.7138, 0.4122,  ..., 0.5306, 0.6128, 0.6811],\n",
       "          ...,\n",
       "          [0.5739, 0.6854, 0.8302,  ..., 0.6582, 0.1045, 0.2559],\n",
       "          [0.3968, 0.6111, 0.3662,  ..., 0.1829, 0.9691, 0.6819],\n",
       "          [0.9268, 0.8808, 0.2251,  ..., 0.3584, 0.1798, 0.7071]],\n",
       "\n",
       "         [[0.7203, 0.4794, 0.7913,  ..., 0.1181, 0.1056, 0.9741],\n",
       "          [0.6668, 0.5308, 0.8848,  ..., 0.7027, 0.2392, 0.8966],\n",
       "          [0.2456, 0.4998, 0.3977,  ..., 0.0245, 0.0396, 0.1409],\n",
       "          ...,\n",
       "          [0.6470, 0.5219, 0.3489,  ..., 0.9408, 0.0520, 0.4333],\n",
       "          [0.8734, 0.5990, 0.9795,  ..., 0.7873, 0.6464, 0.2752],\n",
       "          [0.8499, 0.2333, 0.5675,  ..., 0.0836, 0.9956, 0.2566]],\n",
       "\n",
       "         [[0.5446, 0.3084, 0.3510,  ..., 0.9397, 0.2602, 0.2343],\n",
       "          [0.9580, 0.8142, 0.6473,  ..., 0.1754, 0.6156, 0.8740],\n",
       "          [0.7431, 0.8257, 0.7380,  ..., 0.0137, 0.1053, 0.0122],\n",
       "          ...,\n",
       "          [0.6764, 0.0101, 0.2294,  ..., 0.6091, 0.4599, 0.3375],\n",
       "          [0.3305, 0.4586, 0.5199,  ..., 0.0986, 0.7808, 0.2590],\n",
       "          [0.0853, 0.3204, 0.5270,  ..., 0.1350, 0.3055, 0.8931]]],\n",
       "\n",
       "\n",
       "        [[[0.7619, 0.7265, 0.8650,  ..., 0.4868, 0.7371, 0.0241],\n",
       "          [0.9140, 0.7608, 0.4091,  ..., 0.3932, 0.1419, 0.8823],\n",
       "          [0.6116, 0.2198, 0.0859,  ..., 0.7289, 0.0185, 0.2601],\n",
       "          ...,\n",
       "          [0.9679, 0.5272, 0.8937,  ..., 0.2517, 0.2955, 0.6981],\n",
       "          [0.7074, 0.9487, 0.7572,  ..., 0.6532, 0.9504, 0.8408],\n",
       "          [0.2434, 0.0018, 0.4958,  ..., 0.9492, 0.9122, 0.9534]],\n",
       "\n",
       "         [[0.4929, 0.5324, 0.8480,  ..., 0.0371, 0.3503, 0.5220],\n",
       "          [0.0716, 0.4147, 0.6879,  ..., 0.9249, 0.0054, 0.9219],\n",
       "          [0.2737, 0.0088, 0.3626,  ..., 0.5976, 0.8985, 0.4905],\n",
       "          ...,\n",
       "          [0.7401, 0.6773, 0.1877,  ..., 0.2307, 0.2364, 0.2981],\n",
       "          [0.0452, 0.2130, 0.3768,  ..., 0.6088, 0.6879, 0.2939],\n",
       "          [0.5510, 0.6101, 0.9148,  ..., 0.2858, 0.1572, 0.3616]],\n",
       "\n",
       "         [[0.6640, 0.6151, 0.4577,  ..., 0.8993, 0.9696, 0.0960],\n",
       "          [0.4517, 0.3216, 0.0609,  ..., 0.9580, 0.3617, 0.1823],\n",
       "          [0.8232, 0.9296, 0.8205,  ..., 0.1337, 0.5888, 0.3846],\n",
       "          ...,\n",
       "          [0.8461, 0.3599, 0.8180,  ..., 0.0377, 0.5978, 0.3350],\n",
       "          [0.2847, 0.4172, 0.6325,  ..., 0.9251, 0.8061, 0.1306],\n",
       "          [0.4897, 0.7198, 0.8904,  ..., 0.8411, 0.9856, 0.7545]],\n",
       "\n",
       "         [[0.6640, 0.4645, 0.8259,  ..., 0.0169, 0.2645, 0.7425],\n",
       "          [0.4018, 0.3654, 0.8535,  ..., 0.7339, 0.4465, 0.6761],\n",
       "          [0.3946, 0.2865, 0.3966,  ..., 0.1636, 0.9362, 0.6868],\n",
       "          ...,\n",
       "          [0.6167, 0.4223, 0.5566,  ..., 0.3303, 0.3109, 0.4142],\n",
       "          [0.9469, 0.5176, 0.5703,  ..., 0.9258, 0.0662, 0.2708],\n",
       "          [0.4827, 0.3459, 0.8867,  ..., 0.0938, 0.1831, 0.6550]],\n",
       "\n",
       "         [[0.5470, 0.3836, 0.2415,  ..., 0.8378, 0.3948, 0.1151],\n",
       "          [0.7670, 0.3388, 0.3788,  ..., 0.2969, 0.8175, 0.3563],\n",
       "          [0.9474, 0.0306, 0.7231,  ..., 0.2683, 0.5279, 0.6630],\n",
       "          ...,\n",
       "          [0.4690, 0.8504, 0.7751,  ..., 0.9303, 0.9861, 0.6967],\n",
       "          [0.0521, 0.9774, 0.1539,  ..., 0.2017, 0.8241, 0.8315],\n",
       "          [0.4736, 0.5977, 0.3155,  ..., 0.5106, 0.9640, 0.4276]],\n",
       "\n",
       "         [[0.3245, 0.4727, 0.5099,  ..., 0.9581, 0.1213, 0.8129],\n",
       "          [0.8965, 0.7517, 0.4501,  ..., 0.3719, 0.8133, 0.8822],\n",
       "          [0.1089, 0.5234, 0.3338,  ..., 0.8422, 0.6320, 0.7391],\n",
       "          ...,\n",
       "          [0.5579, 0.0010, 0.3924,  ..., 0.1790, 0.3845, 0.4048],\n",
       "          [0.9949, 0.0752, 0.7176,  ..., 0.1715, 0.5998, 0.2681],\n",
       "          [0.5523, 0.5086, 0.3447,  ..., 0.2002, 0.8553, 0.2795]]],\n",
       "\n",
       "\n",
       "        [[[0.1042, 0.9542, 0.0685,  ..., 0.4935, 0.2060, 0.6770],\n",
       "          [0.3625, 0.8689, 0.6841,  ..., 0.2600, 0.8489, 0.9218],\n",
       "          [0.1000, 0.3163, 0.5364,  ..., 0.2796, 0.0614, 0.4570],\n",
       "          ...,\n",
       "          [0.6607, 0.5122, 0.2837,  ..., 0.3325, 0.1970, 0.9536],\n",
       "          [0.8118, 0.9906, 0.7055,  ..., 0.3705, 0.0305, 0.1511],\n",
       "          [0.4798, 0.1737, 0.8898,  ..., 0.5828, 0.6378, 0.7634]],\n",
       "\n",
       "         [[0.6579, 0.7094, 0.9193,  ..., 0.8641, 0.6441, 0.1551],\n",
       "          [0.6943, 0.8411, 0.2493,  ..., 0.2959, 0.0633, 0.7438],\n",
       "          [0.7683, 0.1341, 0.8612,  ..., 0.3958, 0.8775, 0.0170],\n",
       "          ...,\n",
       "          [0.3617, 0.1066, 0.7755,  ..., 0.2065, 0.7283, 0.3075],\n",
       "          [0.4626, 0.5357, 0.1675,  ..., 0.9491, 0.6266, 0.7061],\n",
       "          [0.2796, 0.0577, 0.2723,  ..., 0.8107, 0.9533, 0.2429]],\n",
       "\n",
       "         [[0.8769, 0.3531, 0.1731,  ..., 0.5169, 0.5744, 0.9959],\n",
       "          [0.0182, 0.6164, 0.0765,  ..., 0.3826, 0.8668, 0.6969],\n",
       "          [0.3488, 0.9650, 0.0219,  ..., 0.6076, 0.4363, 0.1378],\n",
       "          ...,\n",
       "          [0.1585, 0.3016, 0.2490,  ..., 0.6480, 0.6936, 0.0920],\n",
       "          [0.4850, 0.3330, 0.3566,  ..., 0.5740, 0.4747, 0.0820],\n",
       "          [0.3994, 0.8418, 0.8727,  ..., 0.3961, 0.9879, 0.9724]],\n",
       "\n",
       "         [[0.2958, 0.2135, 0.4741,  ..., 0.4624, 0.2750, 0.1007],\n",
       "          [0.0770, 0.9535, 0.1703,  ..., 0.4701, 0.5179, 0.6000],\n",
       "          [0.0595, 0.7578, 0.3461,  ..., 0.0743, 0.4115, 0.6001],\n",
       "          ...,\n",
       "          [0.3274, 0.4093, 0.7118,  ..., 0.6603, 0.2370, 0.8879],\n",
       "          [0.6680, 0.3724, 0.7277,  ..., 0.2378, 0.3286, 0.2003],\n",
       "          [0.5613, 0.3620, 0.1792,  ..., 0.8699, 0.1223, 0.6970]],\n",
       "\n",
       "         [[0.3761, 0.1688, 0.3587,  ..., 0.4238, 0.4212, 0.6160],\n",
       "          [0.2836, 0.1081, 0.0377,  ..., 0.7757, 0.7700, 0.4263],\n",
       "          [0.5214, 0.2830, 0.5620,  ..., 0.8753, 0.7949, 0.2790],\n",
       "          ...,\n",
       "          [0.3786, 0.8832, 0.5060,  ..., 0.6806, 0.7333, 0.9508],\n",
       "          [0.1283, 0.1381, 0.6063,  ..., 0.7220, 0.9656, 0.3307],\n",
       "          [0.8563, 0.4052, 0.2747,  ..., 0.4777, 0.5264, 0.8854]],\n",
       "\n",
       "         [[0.5574, 0.0075, 0.9472,  ..., 0.1791, 0.0717, 0.5911],\n",
       "          [0.7436, 0.6838, 0.1281,  ..., 0.1651, 0.2855, 0.8242],\n",
       "          [0.7285, 0.0274, 0.0716,  ..., 0.0514, 0.6023, 0.3334],\n",
       "          ...,\n",
       "          [0.7145, 0.2096, 0.5752,  ..., 0.9076, 0.5654, 0.8261],\n",
       "          [0.6942, 0.6909, 0.1286,  ..., 0.2405, 0.3621, 0.1493],\n",
       "          [0.6902, 0.5065, 0.1698,  ..., 0.0499, 0.3628, 0.3367]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c127e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.floor_(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f60edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample\"\"\"\n",
    "    def __init__(self, drop_prob: float = 0.):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\"Feed‑forward layer with GELU activation\"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, dropout=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi‑head Self‑Attention\"\"\"\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_dropout=0., proj_dropout=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_dropout)\n",
    "\n",
    "    def forward(self, x: torch.tensor, mask: torch.tensor = None):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x)  # (B, N, 3*C)\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)  # each (B, heads, N, head_dim)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn + mask\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return self.proj_drop(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer Encoder Block with Pre‑Norm, DropPath\"\"\"\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=True,\n",
    "                 drop=0., attn_drop=0., drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads,\n",
    "                              qkv_bias=qkv_bias,\n",
    "                              attn_dropout=attn_drop,\n",
    "                              proj_dropout=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(dim, hidden_dim, dropout=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre‑Norm\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ViT‑style Image Encoder used in BLIP1\n",
    "    (patch embedding + positional embedding + transformer blocks + final norm)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        embed_dim: int = 768,\n",
    "        depth: int = 12,\n",
    "        num_heads: int = 12,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        drop_rate: float = 0.0,\n",
    "        attn_drop_rate: float = 0.0,\n",
    "        drop_path_rate: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        # patch embed\n",
    "        self.patch_embed = nn.Conv2d(3, embed_dim,\n",
    "                                     kernel_size=patch_size,\n",
    "                                     stride=patch_size)\n",
    "        # class token + positional embedding\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth decay rule\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "        # transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias, drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate, drop_path=dpr[i]\n",
    "            )\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # initialize\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.zeros_(m.bias)\n",
    "            nn.init.ones_(m.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (B, 3, H, W)\n",
    "        returns: (B, num_patches+1, embed_dim)  — đầu tiên là CLS token\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        # patch embedding\n",
    "        x = self.patch_embed(x)                   # (B, C, H/ps, W/ps)\n",
    "        x = x.flatten(2).transpose(1, 2)          # (B, N, C)\n",
    "\n",
    "        # prepend cls token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, C)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)          # (B, N+1, C)\n",
    "        x = x + self.pos_embed                         # add pos embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # transformer encoder\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5480f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Stochastic Depth per sample\"\"\"\n",
    "    def __init__(self, drop_prob: float = 0.):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\"Feed-forward layer with GELU\"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, dropout=0.):\n",
    "        super().__init__()\n",
    "        hidden = hidden_features or in_features\n",
    "        out = out_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden, out)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.drop(x)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention\"\"\"\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return self.proj_drop(x)\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"Cross-attention: text queries, image keys/values\"\"\"\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x_text, x_image):\n",
    "        B, Nt, C = x_text.shape\n",
    "        _, Ni, _ = x_image.shape\n",
    "\n",
    "        q = self.q(x_text).reshape(B, Nt, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        kv = self.kv(x_image).reshape(B, Ni, 2, self.num_heads, C // self.num_heads)\n",
    "        k, v = kv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, Nt, C)\n",
    "        x = self.proj(x)\n",
    "        return self.proj_drop(x)\n",
    "\n",
    "class TextBlock(nn.Module):\n",
    "    \"\"\"Transformer block for text (pre-norm)\"\"\"\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., drop=0., attn_drop=0., drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = SelfAttention(dim, num_heads, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = Mlp(dim, int(dim * mlp_ratio), dropout=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"BERT-like unimodal text encoder\"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 max_len: int = 512,\n",
    "                 embed_dim: int = 768,\n",
    "                 depth: int = 12,\n",
    "                 num_heads: int = 12,\n",
    "                 mlp_ratio: float = 4.,\n",
    "                 drop_rate: float = 0.1,\n",
    "                 attn_drop_rate: float = 0.1,\n",
    "                 drop_path_rate: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
    "        self.drop = nn.Dropout(drop_rate)\n",
    "\n",
    "        dpr = torch.linspace(0, drop_path_rate, depth)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TextBlock(embed_dim, num_heads, mlp_ratio,\n",
    "                      drop_rate, attn_drop_rate, dpr[i])\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.max_len = max_len\n",
    "\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: (B, T)\n",
    "        B, T = token_ids.shape\n",
    "        pos = self.pos_embed[:, :T]\n",
    "        x = self.token_embed(token_ids) + pos\n",
    "        x = self.drop(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return x  # (B, T, dim)\n",
    "\n",
    "class ImageGroundedTextEncoder(nn.Module):\n",
    "    \"\"\"Inject cross-attention layers for ITM\"\"\"\n",
    "    def __init__(self,\n",
    "                 base_encoder: TextEncoder,\n",
    "                 cross_interval: int = 1):\n",
    "        super().__init__()\n",
    "        self.base = base_encoder\n",
    "        self.cross_idx = set(range(0, len(self.base.blocks), cross_interval))\n",
    "        # make cross-attention modules\n",
    "        self.cross_layers = nn.ModuleDict({\n",
    "            str(i): CrossAttention(self.base.token_embed.embedding_dim,\n",
    "                                   self.base.blocks[0].attn.num_heads)\n",
    "            for i in self.cross_idx\n",
    "        })\n",
    "        self.itm_head = nn.Linear(self.base.token_embed.embedding_dim, 1)\n",
    "\n",
    "    def forward(self, token_ids, image_feats):\n",
    "        # token_ids: (B, T), image_feats: (B, N_img, dim)\n",
    "        x = self.base.token_embed(token_ids) + self.base.pos_embed[:, :token_ids.size(1)]\n",
    "        x = self.base.drop(x)\n",
    "        for i, blk in enumerate(self.base.blocks):\n",
    "            x = blk(x)\n",
    "            if i in self.cross_idx:\n",
    "                x = x + self.cross_layers[str(i)](x, image_feats)\n",
    "        x = self.base.norm(x)\n",
    "        # ITM: use first token ([Encode])\n",
    "        cls_feat = x[:, 0]\n",
    "        itm_logit = self.itm_head(cls_feat).squeeze(-1)\n",
    "        return x, itm_logit\n",
    "\n",
    "# Example instantiation:\n",
    "# text_encoder = TextEncoder(vocab_size=30522)\n",
    "# img_encoder = ImageEncoder()\n",
    "# ig_text_encoder = ImageGroundedTextEncoder(text_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ddc3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cls_token = torch.nn.Parameter(torch.zeros(1,1,768))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "178d4a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0.]]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "155b9908",
   "metadata": {},
   "outputs": [],
   "source": [
    "b= cls_token.expand(8,-1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14d5f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(8,16,768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fad38590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 17, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8102083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natmin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
